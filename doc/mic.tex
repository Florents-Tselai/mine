\documentclass[12pt]{article}
% Formatting: https://www.informs.org/Recognize-Excellence/INFORMS-Prizes-Awards/Undergraduate-Operations-Research-Prize/INFORMS-Undergraduate-Operations-Research-Prize-Application-Process
\usepackage[a4paper,left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[cmex10]{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{lscape}



\begin{document}

\title{Computing the Maximal Information Coefficient:\\A Metaheuristic Approach}
\author{
Florents Tselai\\
Department of Management Science \& Technology\\
Athens University of Economics \& Business\\
\href{mailto:tselai@dmst.aueb.gr}{tselai@dmst.aueb.gr}
}
\date{}
\maketitle

\begin{abstract}
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?
\end{abstract}




\section{Introduction}

Detecting both novel
and non-obvious associations in large datasets
is the cornerstoe of effective decieison support and meaningfull insights.
Discovering hidden realtions and patters lying in the data
can be an invaluable tool for information retrieval and knowledge mining.

But how does one search and find these relationships?
Researchers from many fields have been trying to answer
this question for years. Unfurtunately,
the majority of the methods they have came up with
are of limited scope and applications.
Most of them detect only specific types of associations (e.g. linear funcional relationships).

In real-life scenarios however the relationships are far more complex and non-intuitive.
Well-known metrics like Pearson's and Spearman's correlation coefficients fail in these cases.
This gives birth to the need for methdos and metrics that are able to capture a wide range of relationships
(functional or not).

\cite{reshef2011detecting} propose a new metric named {\it Maximal Information Coefficient (MIC)}
that can effectively capture a wide reange of associations (functional or not).
According to the authors MIC preserves two important properties.
{\it Generality} refers to the act that wwith sufficient sample size MIC can capture a wide range of interesting associations,
not limied to specific function types (such as linear, exponential, or periodci), or even functional relationships.
{\it Equitability} means that MIC gives similar scores to equally noisy relationships of different types.

\cite{cho2012human, koren2012host, speed2011correlation}

The rest of the paper is structured as follows:
in section~\ref{sec:formulation} we establish the notation to be used,
describe briefly the original algorithm proposed by \cite{reshef2011detecting}
and formulate the problem of computing the MIC as a mathematical programming problem.
In section~\ref{sec:algorithm} we present our metaheuristic algorithm to approximate the characteristic matrix,
as an alternative to the original method proposed by \cite{reshef2011detecting}.
Section~\ref{sec:experiments} provides detailed computational experiments
conducted with our algorithm and compares it with the original algorithm
Section~\ref{sec:issues} discusses some issues suitable for further future research.
Finally, section~\ref{sec:conclusion} concludes the paper.




\section{Definitions \& Problem Formulation}
\label{sec:formulation}
\subsection{Definitions}
\footnote{To the extent possible we faithfully follow the notation used in \cite{reshef2011detecting}}
Given a dataset $D$ of $n$ ordered pairs $(x_i,y_i), i=1,\ldots,n$,
we can partition the $x$-values of $D$ in $x$ bins and the $y$-values in $y$ bins allowing empty bins.
We call such a pair of partitions a $x-by-y$ grid $G$. Given the grid $G$, let $D|_G$ be the distributions of points of $D$ in the cells of $G$
by letting the probability mass of each cell be the fraction of points in $D$ falling in that cell.

For a finite set $D$ and positive integers $x,y$ we define $$I^*(D,x,y)=\max I(D|_G)$$
where $I^*(D,x,y)$ is the maximum mutual information achieved by any grid with $x$ rows and $y$ columns and $I(D|_G)$ denotes the mutual information of $D|_G$

The {\it characteristic matrix} $M(D)$ of a set $D$ of two-variable data is an infinite matrix with entries $$M(D)_{x,y}=\frac{I^*(D,x,y)}{\log \min\{x,y\}}$$.
Each entry $M(D)_{x,y}$ stores the {\it optimal grid} with $x$ rows and $y$ columns. That is the grid wi

The {\it Maximal Information Coefficient (MIC)} of a set $D$ of two-variable data with sample size $n$ and grid size less than $B(n)$ is given by
$$MIC(D)=\smash{\displaystyle\max_{xy<B(n)}} \{M(D)_{x,y}\}$$

The {\it Maximum Asymmetry Score (MAS)} is defined by $$MAS(D)=\smash{\displaystyle\max_{xy<B(n)}} |M(D)_{x,y}-M(D)_{y,x}|$$
and measures deviation from monotonicity.

The {\it Maximum Edge Value (MEV)} is defined by $$MEV(D)=\smash{\displaystyle\max_{xy<B(n)}} \{M(D)_{x,y}:x=2\ or\ y=2\}$$
and measures the degree to which the dataset appears to be sampled from a continuous function.

The {\it Minimum Cell Number (MCN)} is defined by $$MCN(D,\epsilon)=\smash{\displaystyle\min_{xy<B(n)}} \{\log(xy):M(D)_{x,y}\geq(1-\epsilon)MIC(D)\}$$
and measures the complexity of the association, in terms of the number of cells required to reach the MIC score.

Each grid $G$ is comprised of a x-axis partition $P$ and a y-axis partition $Q$
and yields a mutual information score $$I(G)=I(P;Q)=H(P)+H(Q)-H(P,Q)$$ where $H$ denotes shannon entropy.

Let $D_x$ and $D_y$ denote a view of $D$ in increasing order by x-value and y-value accordingly.

\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{examples/example_grid.png}}
  \caption{Example partitions}
  \label{fig:example_grid}
\end{figure}



\section{Metaheuristic Algorithm}
\label{sec:algorithm}
\subsection{Solution Representation}
Each grid corresponds to a solution.
Each solution $S$ consists of a pair of a x-axis and a y-axis partition $S=\langle P, Q \rangle$
$P=(p_0,p_1,\ldots,p_l)$ is a x-axis partition of size $l$
$Q=(q_0,q_1,\ldots,q_m)$ is a y-axis partition of size $m$

\subsection{Objective Function}
Objective function
$f:\langle P,Q \rangle \mapsto [0,1]$



\begin{algorithm}
\begin{algorithmic}[1]
\caption{CharacteristicMatrix$(D,B)$}
\label{CharacteristicMatrix}
\REQUIRE $D$ is a set of ordered pairs
\REQUIRE $B$ is an integer greater than 3
\FOR{$(x,y)$ such that $xy\leq B$}
	\STATE $I_{x,y}$ $\gets$ MaxMI$(D,x,y)$
	\STATE $M_{x,y}$ $\gets$ $I_{x,y}/\min \{\log x, \log y\}$
\ENDFOR
\RETURN $\{M_{x,y}:xy\leq B\}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1]
\caption{ApproxMaxMI$(D,x,y,\hat{k})$}
\label{ApproxMaxMI}
\REQUIRE $D$ is a set of ordered pairs
\REQUIRE $x,y,\hat{k}$ are integers greater than 1
\ENSURE Returns a set of mutual information scores $(I_{2,y},\ldots,I_{x,y})$ such that $I_{i,j}$ is heuristically cose to the highest achievable mutual information score using $i$ rows and $j$ columns
\STATE $Q \gets$ EquipartitionYAxis$(D,y)$
\STATE $D \gets$ SortInIncreasingOrderByXValue$(D)$
\RETURN ApproxOptimizeXAxis$(D,Q,x,\hat{k})$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1]
\caption{ApproxCharacteristicMatrix$(D,B,c)$}
\label{ApproxCharacteristicMatrix}
\REQUIRE $D={(a_1,b_1),\ldots,(a_n,b_n)}$ is a set of ordered pairs
\REQUIRE $B$ is an integer greater than 3
\REQUIRE $c$ is greater than 0
\STATE $D^{\perp} \gets \{(b_1,a_1),\ldots,(b_n,a_n)\}$
\FORALL {$y\in\{2,\ldots,\lfloor B/2\rfloor\}$}
	\STATE $x \gets \lfloor B/y\rfloor$
	\STATE $(I_{2,y},\ldots ,I_{x,y}) \gets $ ApproxMaxMI$(D,x,y,cx)$
	\STATE $({I^{\perp}}_{2,y},\ldots ,{I^{\perp}}_{x,y}) \gets $ ApproxMaxMI$(D^{\perp},x,y,cx)$
\ENDFOR
\FOR{$(x,y)$ such that $xy\leq B$}
	\STATE $I_{x,y} \gets \max \{I_{x,y},{I^{\perp}}_{y,x}\}$
	\STATE $M_{x,y} \gets I_{x,y}/\min \{\log x, \log y\}$
\ENDFOR
\RETURN $\{M_{x,y}:xy\leq B\}$
\end{algorithmic}
\end{algorithm}



\section{Computational Results \& Experiments}
\label{sec:experiments}

%\begin{landscape}
\begin{table}[H]
\centering
\input{experiments/experiment_1.tex}
\caption{Running time for common function for various number of points}
\label{table:kysymys}
\end{table}
%\end{landscape}



\begin{figure}
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{experiments/experiment_1_plot.png}}%
  \caption{Running time for common function for various number of points}
  \label{fig:key}
\end{figure}

\section{Issues \& Further Research}
\label{sec:issues}

\section{Conclusions}
\label{sec:conclusion}

The conclusion goes here.

\section*{Acknowledgment}
The authors would like to thank...

\section*{Appendix}



\begin{table}[H]
\centering
\input{experiments/functions_definitions.tex}
\caption{Definitions of functions used during the experiments}
\label{table:functions_definitions}
\end{table}


\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,mic}

\end{document}



